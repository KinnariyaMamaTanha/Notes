# 一、关键组件

机器学习的关键组件包括了四项：
1. data：可用于学习的数据
2. model：用于转换数据的模型
3. objective function：目标函数，用于量化模型的有效性
4. algorithm：调整模型参数以优化目标函数的算法

## 1.1 Data

1. 每个数据集都由样本(example, sample)组成，大多时候满足[独立同分布](https://zhuanlan.zhihu.com/p/52530189)，
2. 样本又称作数据点 （data point）或者数据实例（data instance），通常每个样本由一组称为特征（features，或协变量（covariates）） 的属性组成
3. 当每个样本的特征类别数量都相同时，其特征向量是同长度的，该长度被称为数据的维数（dimensionality）（但是往往不会同长度，这时候可以使用深度学习）
4. 数据越多且正确率越高，越好
5. 数据集分为训练集和测试集

## 1.2 Model

大多数机器学习都会涉及到数据的转换

## 1.3 Objective Function

1. 数据转换后需要一定的标准来度量其优劣，因此需要用目标函数。一般希望优化到目标函数到最低点（所以又被称为损失函数 loss function），也可以选择优化越高越好。
2. 试图预测数值时，最常见的损失函数是平方误差（类似方差中的处理）；试图解决分类问题时，最常见的是最小化错误率

## 1.4 Algorithm

当获得了上述三者后，希望有一个算法能够找到最优的参数，使得最小化损失函数。通常基于**梯度下降法**（对一个位置的变量进行轻微扰动，看往哪个方向会变小，再向那个方向调整，以前做数学竞赛的时候也有类似的题目使用这种想法）

# 二、几种类别

## 2.1 Supervised Learning

>*监督学习擅长在“给定输入特征”的情况下预测标签*

监督学习适用于需要将任何未知的输入特征映射到**标签**上的情况，如预测患者的心脏病是否会发作等

监督学习之所以能发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实 的标签

监督学习可分为三大步骤：
1. 从已知大量数据样本中随机选取一个子集，为每个样本获取**真实**标签。有时，这些样本已有标签（例如， 患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集； 
2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”； 
3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。
![](http://zh-v2.d2l.ai/_images/supervised-learning.svg)

一些监督学习的实例：
1. **回归问题**：当标签取任意数值时，称为回归问题（回忆回归方程）。此时，目标为生成一个很好的模型，其标签的预测值和实际值很接近
2. **分类问题**：希望通过新的样本特征得出它属于哪个类别（如图像识别），和回归问题很相似。最简单的是二项分类，其次为多项分类、层次分类等。分类问题的常见损失函数被称为交叉熵。
	1. **二项分类**指只有两种类别的分类
	2. **多项分类**指有多种类且分类错误的“代价”是均等的情况（如识别数字）
	3. **层次分类**指有多种类且类之间拥有联系（这将导致错误的成本有所不同，如把一种狗识别为另一种狗的 “代价” 小于识别为恐龙的 “代价” ）
3. **标记问题**：学习预测不相互排斥的类别的问题称为多标签分类（如博客中的标签一般并不互相排斥），即一个样本可以映射到多个标签中
4. **搜索问题**：如搜索引擎
5. **推荐系统**：如电商平台
6. **序列学习**：当输入的样本是连续的时候，可能可以从前一个样本推断出后一个样本所映射到的标签，这时如果有记忆能力，则可以增加预测的准确度，如机器翻译、语音转文本、文本转语音

## 2.2 Unsupervised Learning

>*在监督学习中，给定了一系列的样本特征到标签的映射，要求从中推断出未知样本所映射到的标签；而在无监督学习中，并没有上述给定的映射，需要机器 “自发” 的学习*

一些无监督学习的实例：
1. **聚类问题**：在没有标签的情况下，为数据分类
2. **主成分分析**：尝试找到少量参数来估计数据的线性相关属性（如衣服的尺码）
3. **因果关系和概率图模型问题**：根据数据发现其中的关系，描述可能产生这些关系的原因
4. **生成对抗性网络**：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试

## 2.3 Reinforcement Learning

>*在监督学习和无监督学习中，都是先获得大量数据，再生成模型，最后启用它，不再与环境交互，称为离线学习*

强化学习将与环境交互，如AlphaGo等

在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得奖励（reward）。此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。

![](http://zh-v2.d2l.ai/_images/rl-environment.svg)

可将任何监督学习问题转化为强化学习问题，如在分类问题中，可以创建一个强化学习智能体，每个分类对应一个“动作”。然后，我们可以创建一个环境，该环境给予智能体的奖励。这个奖励与原始监督学习问题的损失函数是一致的。

强化学习还可以完成监督学习不能完成的任务。在强化学习中，我们并不假设环境告诉智能体每个观测的最优动作。一般来说，智能体只是得到一些奖励。此外，环境甚至可能不会告诉是哪些行为导致了奖励。

强化学习可能还必须处理部分可观测性问题。即有时无法观测到整个环境。当环境可被完全观察到时，强化学习问题被称为马尔可夫决策过程（markov decision process）

最后，在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。 强化学习智能体必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）。
